version: '3.8'

services:
    # Ollama service for running LLM
    ollama:
        image: ollama/ollama:latest
        container_name: ollama
        ports:
            - '11434:11434'
        volumes:
            - ollama_data:/root/.ollama
        environment:
            - OLLAMA_KEEP_ALIVE=24h
            - OLLAMA_MAX_LOADED_MODELS=1
            - OLLAMA_NUM_PARALLEL=2
        restart: unless-stopped
        deploy:
            resources:
                limits:
                    memory: 4G
                    cpus: '2'
        healthcheck:
            test: ['CMD', 'ollama', 'list']
            interval: 30s
            timeout: 10s
            retries: 3

    # Backend BFF service
    backend:
        build:
            context: ./packages/server
            dockerfile: Dockerfile
        container_name: backend-bff
        ports:
            - '3001:3001'
        environment:
            - PORT=3001
            - OLLAMA_URL=http://ollama:11434
            - MODEL_NAME=phi4-mini
            - USE_OLLAMA=true
            - FRONTEND_URL=http://localhost:5173
            - NODE_ENV=production
        depends_on:
            - ollama
        restart: unless-stopped
        volumes:
            - ./packages/server:/app
            - /app/node_modules
        healthcheck:
            test: ['CMD', 'curl', '-f', 'http://localhost:3001/health']
            interval: 30s
            timeout: 10s
            retries: 3

    # Frontend service
    frontend:
        build:
            context: ./packages/web
            dockerfile: Dockerfile
        container_name: frontend
        ports:
            - '5173:5173'
        environment:
            - VITE_API_BASE_URL=http://localhost:3001
        depends_on:
            - backend
        restart: unless-stopped
        volumes:
            - ./packages/web:/app
            - /app/node_modules

volumes:
    ollama_data:
        driver: local

networks:
    default:
        driver: bridge
